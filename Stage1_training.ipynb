{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e711f-97be-431a-8f5a-65d2210c4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/content/birds/\"\n",
    "train_dir = data_dir + \"/train\"\n",
    "test_dir = data_dir + \"/test\"\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "stage1_generator_lr = 0.0002\n",
    "stage1_discriminator_lr = 0.0002\n",
    "stage1_lr_decay_step = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b813c2-5c8a-4aba-bbe0-2efa8e1eaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "condition_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd013260-578f-4c26-9489-d7c5470473ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "\n",
    "filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "\n",
    "cub_dataset_dir = \"/content/CUB_200_2011\"\n",
    "\n",
    "# Define optimizers\n",
    "dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83501f-4990-47f1-bb9d-c3f13ee517ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Load datasets\n",
    "\"\"\"\n",
    "X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
    "                                                  class_info_file_path=class_info_file_path_train,\n",
    "                                                  cub_dataset_dir=cub_dataset_dir,\n",
    "                                                  embeddings_file_path=embeddings_file_path_train,\n",
    "                                                  image_size=(64, 64))\n",
    "\n",
    "X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
    "                                               class_info_file_path=class_info_file_path_test,\n",
    "                                               cub_dataset_dir=cub_dataset_dir,\n",
    "                                               embeddings_file_path=embeddings_file_path_test,\n",
    "                                               image_size=(64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17968d7f-be3f-4ee7-903e-95edfed04017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build and compile networks\n",
    "\"\"\"\n",
    "ca_model = build_ca_model()\n",
    "ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "stage1_dis = build_stage1_discriminator()\n",
    "stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "stage1_gen = build_stage1_generator()\n",
    "stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "embedding_compressor_model = build_embedding_compressor_model()\n",
    "embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
    "adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
    "                          optimizer=gen_optimizer, metrics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d4231-e3cc-43df-afc9-712f4854003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
    "tensorboard.set_model(stage1_gen)\n",
    "tensorboard.set_model(stage1_dis)\n",
    "tensorboard.set_model(ca_model)\n",
    "tensorboard.set_model(embedding_compressor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb9efe-91e8-444a-add2-265779eedc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array containing real and fake values\n",
    "# Apply label smoothing as well\n",
    "real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98412ba-4688-49d7-b159-092d7496bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"========================================\")\n",
    "    print(\"Epoch is:\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "\n",
    "    # Load data and train model\n",
    "    number_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    for index in range(number_of_batches):\n",
    "        print(\"Batch:{}\".format(index+1))\n",
    "\n",
    "        \"\"\"\n",
    "        Train the discriminator network\n",
    "        \"\"\"\n",
    "        # Sample a batch of data\n",
    "        z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "        image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
    "\n",
    "        # Generate compressed embeddings\n",
    "        compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
    "        compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
    "        compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "\n",
    "        dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
    "                                                  np.reshape(real_labels, (batch_size, 1)))\n",
    "        dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
    "                                                  np.reshape(fake_labels, (batch_size, 1)))\n",
    "        dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
    "                                                   np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "\n",
    "        d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
    "\n",
    "        print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "        print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "        print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "        print(\"d_loss:{}\".format(d_loss))\n",
    "\n",
    "        \"\"\"\n",
    "        Train the generator network \n",
    "        \"\"\"\n",
    "        g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "        print(\"g_loss:{}\".format(g_loss))\n",
    "\n",
    "        dis_losses.append(d_loss)\n",
    "        gen_losses.append(g_loss)\n",
    "\n",
    "    \"\"\"\n",
    "    Save losses to Tensorboard after each epoch\n",
    "    \"\"\"\n",
    "    write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "\n",
    "    # Generate and save images after every 2nd epoch\n",
    "    if epoch % 2 == 0:\n",
    "        # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "        z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        embedding_batch = embeddings_test[0:batch_size]\n",
    "        fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n",
    "\n",
    "        # Save images\n",
    "        for i, img in enumerate(fake_images[:10]):\n",
    "            save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eced06-4280-4b31-95da-7fbe207f6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "stage1_gen.save_weights(\"stage1_gen.h5\")\n",
    "stage1_dis.save_weights(\"stage1_dis.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl_310:Python",
   "language": "python",
   "name": "conda-env-ml_dl_310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
